import os
import sys
import tensorflow as tf

BASE_DIR = os.path.dirname(os.path.abspath(__file__)) 
sys.path.append(BASE_DIR) 
sys.path.append(os.path.join(BASE_DIR, 'utils')) 


def discriminative_loss_single(prediction, correct_label, feature_dim,
                               delta_v, delta_d, param_var, param_dist, param_reg):
    ''' Discriminative loss for a single prediction/label pair.
    :param prediction: inference of network
    :param correct_label: instance label
    :feature_dim: feature dimension of prediction
    :param label_shape: shape of label
    :param delta_v: cutoff variance distance
    :param delta_d: curoff cluster distance
    :param param_var: weight for intra cluster variance
    :param param_dist: weight for inter cluster distances
    :param param_reg: weight regularization
    '''

    ### Reshape so pixels are aligned along a vector
    #correct_label = tf.reshape(correct_label, [label_shape[1] * label_shape[0]])
    #å°†predictionæ”¹å˜ç»´åº¦ æ”¹ä¸ºN*5
    reshaped_pred = tf.reshape(prediction, [-1, feature_dim])

    ### Count instances
    #unique_labelsæ˜¯å®ä¾‹ç§ç±»[1,2,3]å°±æ˜¯æœ‰ä¸‰ä¸ªå®ä¾‹ï¼Œunique_idæ˜¯ä¸ªcorrect_labelç­‰é•¿çš„å¼ é‡ï¼Œç”¨æ¥æ˜¾ç¤ºcorrect_labelä¸­æ¯ä¸€ä½?
    #å±äºå“ªä¸ªå®ä¾‹ï¼Œcountsæ¯ä¸ªå®ä¾‹ä¸­æœ‰å¤šå°‘ç‚?
    unique_labels, unique_id, counts = tf.unique_with_counts(correct_label)
    #æ•°æ®ç±»å‹è½¬æ¢
    counts = tf.cast(counts, tf.float32)
    #num_instancesæ˜¯å®ä¾‹ä¸ªæ•?
    num_instances = tf.size(unique_labels)
    
    #å°†å±äºåŒä¸€ä¸ªå®ä¾‹çš„ç‚¹æ‰€é¢„æµ‹å‡ºæ¥çš„ç‰¹å¾å‘é‡ç›¸åŠ ï¼Œç”Ÿæˆä¸€ä¸ªnum_instances*5çš„çŸ©é˜?
    segmented_sum = tf.unsorted_segment_sum(reshaped_pred, unique_id, num_instances)

    #ç”¨ä¸Šè¿°åŠ å’Œé™¤ä»¥gtçš„æ¯ä¸ªå®ä¾‹çš„ç‚¹çš„æ•°é‡ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„num_instances*5çš„çŸ©é˜µï¼Œæ˜¯æœ¬æ¬¡é¢„æµ‹ä¸­æ¯ä¸ªå®ä¾‹ä¸­çš„æ‰€æœ‰ç‚¹çš„ç‰¹å¾å‘é‡çš„å¹³å‡å€?
    mu = tf.div(segmented_sum, tf.reshape(counts, (-1, 1)))
    #å°†è¿™ä¸ªå‡å€¼åˆ†é…ç»™æ¯ä¸ªå®ä¾‹ä¸­çš„ç‚¹ï¼Œå¦‚å®ä¾?çš„ç‚¹çš„ç‰¹å¾å‘é‡éƒ½æ˜¯ä¹‹å‰ç®—çš„å¹³å‡å€?
    mu_expand = tf.gather(mu, unique_id)

    ### Calculate l_var
    #distance = tf.norm(tf.subtract(mu_expand, reshaped_pred), axis=1)
    #tmp_distance = tf.subtract(reshaped_pred, mu_expand)
    #ä¸‹é¢ä¸¤è¡Œæ˜¯ç®—reshaped_predå’Œmu_expandçš„è·ç¦»çš„ä¸€èŒƒæ•°ï¼Œè¿˜æ˜¯ä¸€ä¸ªN*1çš„çŸ©é˜?
    tmp_distance = reshaped_pred - mu_expand
    #æ±‚è¡Œä¸€èŒƒæ•°
    distance = tf.norm(tmp_distance, ord=1, axis=1)

    distance = tf.subtract(distance, delta_v)
    #å°†distanceçš„æ•°æ®å€¼é™åˆ¶åœ¨0åˆ°distance
    distance = tf.clip_by_value(distance, 0., distance)
    distance = tf.square(distance)

    #æŠŠGTä¸­å±äºåŒä¸€å®ä¾‹çš„ç‚¹ç®—å‡ºæ¥çš„æ–°çš„è·ç¦»å·®åŠ å’?
    l_var = tf.unsorted_segment_sum(distance, unique_id, num_instances)
    l_var = tf.div(l_var, counts)
    #å°†l_varåŠ å’Œå¹¶å˜æˆä¸€ä¸ªå€?
    l_var = tf.reduce_sum(l_var)
    #é™¤å®ä¾‹ä¸ªæ•?
    l_var = tf.divide(l_var, tf.cast(num_instances, tf.float32))

    ### Calculate l_dist

    # Get distance for each pair of clusters like this:
    #   mu_1 - mu_1
    #   mu_2 - mu_1
    #   mu_3 - mu_1
    #   mu_1 - mu_2
    #   mu_2 - mu_2
    #   mu_3 - mu_2
    #   mu_1 - mu_3
    #   mu_2 - mu_3
    #   mu_3 - mu_3
    #å°†åŸæ¥çš„num_instances*5çš„çŸ©é˜µï¼Œè¡Œæ‰©å±•num_instanceså€ï¼Œåˆ—ä¸å?
    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])
    # å°†åŸæ¥çš„num_instances*5çš„çŸ©é˜µï¼Œåˆ—æ‰©å±•num_instanceså€ï¼Œè¡Œä¸å?
    mu_band_rep = tf.tile(mu, [1, num_instances])
    #å°†æ‰©å±•åçš„çŸ©é˜µå˜æˆnum_instances * num_instancesè¡?åˆ—çš„çŸ©é˜µï¼ŒçŸ©é˜µæ²¡num_instancesè¡Œæ˜¯ç›¸åŒçš„ï¼Œå¦‚å‰num_instancesè¡Œç›¸å?
    mu_band_rep = tf.reshape(mu_band_rep, (num_instances * num_instances, feature_dim))

    #åšå·®
    mu_diff = tf.subtract(mu_band_rep, mu_interleaved_rep)

    # Filter out zeros from same cluster subtraction
    #ç”Ÿæˆä¸€ä¸ªnum_instances * num_instancesçš„å•ä½çŸ©é˜?
    eye = tf.eye(num_instances)
    zero = tf.zeros(1, dtype=tf.float32)
    #å°†å•ä½çŸ©é˜µå˜æˆå¸ƒå°”ç±»å‹ï¼Œ1å˜æˆfalseï¼?å˜æˆtrue
    diff_cluster_mask = tf.equal(eye, zero)
    #å°†çŸ©é˜µå˜å½¢æˆä¸€ä¸ªè¡Œå‘é‡(ä¸€ç»?
    diff_cluster_mask = tf.reshape(diff_cluster_mask, [-1])
    #tf.boolean_maskå‰ä¸€ä¸ªçŸ©é˜µä¿ç•™åä¸€ä¸ªçŸ©é˜µä¸­ä¸ºtrueçš„éƒ¨åˆ?æ­¤å¤„æ˜¯æŠŠå…¨ä¸ºé›¶çš„è¡Œå‡å?
    mu_diff_bool = tf.boolean_mask(mu_diff, diff_cluster_mask)

    #intermediate_tensor = tf.reduce_sum(tf.abs(mu_diff),axis=1)
    #zero_vector = tf.zeros(1, dtype=tf.float32)
    #bool_mask = tf.not_equal(intermediate_tensor, zero_vector)
    #mu_diff_bool = tf.boolean_mask(mu_diff, bool_mask)

    #æ±‚è¡Œä¸€èŒƒæ•°
    mu_norm = tf.norm(mu_diff_bool, ord=1, axis=1)
    mu_norm = tf.subtract(2. * delta_d, mu_norm)
    #max(0,mu_norm)
    mu_norm = tf.clip_by_value(mu_norm, 0., mu_norm)
    mu_norm = tf.square(mu_norm)

    l_dist = tf.reduce_mean(mu_norm)

    def rt_0(): return 0.
    def rt_l_dist(): return l_dist
    l_dist = tf.cond(tf.equal(1, num_instances), rt_0, rt_l_dist)
    
    ### Calculate l_reg
    l_reg = tf.reduce_mean(tf.norm(mu, ord=1, axis=1))

    param_scale = 1.
    l_var = param_var * l_var
    l_dist = param_dist * l_dist
    l_reg = param_reg * l_reg

    loss = param_scale * (l_var + l_dist + l_reg)

    return loss, l_var, l_dist, l_reg


def discriminative_loss(prediction, correct_label, feature_dim,
                        delta_v, delta_d, param_var, param_dist, param_reg):
    ''' Iterate over a batch of prediction/label and cumulate loss
    :return: discriminative loss and its three components
    '''

    def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):
        return tf.less(i, tf.shape(batch)[0])

    def body(label, batch, out_loss, out_var, out_dist, out_reg, i):
        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(prediction[i], correct_label[i], feature_dim,
                                                                     delta_v, delta_d, param_var, param_dist, param_reg)

        out_loss = out_loss.write(i, disc_loss)
        out_var = out_var.write(i, l_var)
        out_dist = out_dist.write(i, l_dist)
        out_reg = out_reg.write(i, l_reg)

        return label, batch, out_loss, out_var, out_dist, out_reg, i + 1

    # TensorArray is a data structure that support dynamic writing
    output_ta_loss = tf.TensorArray(dtype=tf.float32,
                                    size=0,
                                    dynamic_size=True)
    output_ta_var = tf.TensorArray(dtype=tf.float32,
                                   size=0,
                                   dynamic_size=True)
    output_ta_dist = tf.TensorArray(dtype=tf.float32,
                                    size=0,
                                    dynamic_size=True)
    output_ta_reg = tf.TensorArray(dtype=tf.float32,
                                   size=0,
                                   dynamic_size=True)

    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _ = tf.while_loop(cond, body, [correct_label,
                                                                                           prediction,
                                                                                           output_ta_loss,
                                                                                           output_ta_var,
                                                                                           output_ta_dist,
                                                                                           output_ta_reg,
                                                                                           0])
    out_loss_op = out_loss_op.stack()
    out_var_op = out_var_op.stack()
    out_dist_op = out_dist_op.stack()
    out_reg_op = out_reg_op.stack()

    disc_loss = tf.reduce_mean(out_loss_op)
    l_var = tf.reduce_mean(out_var_op)
    l_dist = tf.reduce_mean(out_dist_op)
    l_reg = tf.reduce_mean(out_reg_op)

    return disc_loss, l_var, l_dist, l_reg


def discriminative_loss_single_multicate(sem_label, prediction, correct_label, feature_dim,
                               delta_v, delta_d, param_var, param_dist, param_reg):
    ''' Discriminative loss for a single prediction/label pair.
    :param sem_label: semantic label
    :param prediction: inference of network
    :param correct_label: instance label
    :feature_dim: feature dimension of prediction
    :param label_shape: shape of label
    :param delta_v: cutoff variance distance
    :param delta_d: curoff cluster distance
    :param param_var: weight for intra cluster variance
    :param param_dist: weight for inter cluster distances
    :param param_reg: weight regularization
    '''
    unique_sem_label, unique_id, counts = tf.unique_with_counts(sem_label)
    num_sems = tf.size(unique_sem_label)

    def cond(i, ns, unique_id, pred, ins_label, out_loss, out_var, out_dist, out_reg):
        return tf.less(i, num_sems)

    def body(i, ns, unique_id, pred, ins_label, out_loss, out_var, out_dist, out_reg):
        inds = tf.equal(i, unique_id)
        cur_pred = tf.boolean_mask(prediction, inds)
        cur_label = tf.boolean_mask(correct_label, inds)
        cur_discr_loss, cur_l_var, cur_l_dist, cur_l_reg = discriminative_loss_single(cur_pred, cur_label, feature_dim,
                            delta_v, delta_d, param_var, param_dist, param_reg)
        out_loss = out_loss.write(i, cur_discr_loss)
        out_var = out_var.write(i, cur_l_var)
        out_dist = out_dist.write(i, cur_l_dist)
        out_reg = out_reg.write(i, cur_l_reg)

        return i + 1, ns, unique_id, pred, ins_label, out_loss, out_var, out_dist, out_reg

    output_ta_loss = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
    output_ta_var = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
    output_ta_dist = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
    output_ta_reg = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)

    loop = [0, num_sems, unique_id, prediction, correct_label, output_ta_loss, output_ta_var, output_ta_dist, output_ta_reg]
    _, _, _, _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op = tf.while_loop(cond, body, loop)

    out_loss_op = out_loss_op.stack()
    out_var_op = out_var_op.stack()
    out_dist_op = out_dist_op.stack()
    out_reg_op = out_reg_op.stack()

    disc_loss = tf.reduce_sum(out_loss_op)
    l_var = tf.reduce_sum(out_var_op)
    l_dist = tf.reduce_sum(out_dist_op)
    l_reg = tf.reduce_sum(out_reg_op)

    return disc_loss, l_var, l_dist, l_reg


def discriminative_loss_multicate(sem_label, prediction, correct_label, feature_dim,
                        delta_v, delta_d, param_var, param_dist, param_reg):
    ''' Iterate over a batch of prediction/label and cumulate loss for multiple categories.
    :return: discriminative loss and its three components
    '''

    def cond(sem, label, batch, out_loss, out_var, out_dist, out_reg, i):
        return tf.less(i, tf.shape(batch)[0])

    def body(sem, label, batch, out_loss, out_var, out_dist, out_reg, i):
        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single_multicate(sem_label[i], prediction[i], correct_label[i], feature_dim,
                                                                     delta_v, delta_d, param_var, param_dist, param_reg)

        out_loss = out_loss.write(i, disc_loss)
        out_var = out_var.write(i, l_var)
        out_dist = out_dist.write(i, l_dist)
        out_reg = out_reg.write(i, l_reg)

        return sem, label, batch, out_loss, out_var, out_dist, out_reg, i + 1

    # TensorArray is a data structure that support dynamic writing
    output_ta_loss = tf.TensorArray(dtype=tf.float32,
                                    size=0,
                                    dynamic_size=True)
    output_ta_var = tf.TensorArray(dtype=tf.float32,
                                   size=0,
                                   dynamic_size=True)
    output_ta_dist = tf.TensorArray(dtype=tf.float32,
                                    size=0,
                                    dynamic_size=True)
    output_ta_reg = tf.TensorArray(dtype=tf.float32,
                                   size=0,
                                   dynamic_size=True)

    _, _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _ = tf.while_loop(cond, body, [sem_label,
                                                                                           correct_label,
                                                                                           prediction,
                                                                                           output_ta_loss,
                                                                                           output_ta_var,
                                                                                           output_ta_dist,
                                                                                           output_ta_reg,
                                                                                           0])
    out_loss_op = out_loss_op.stack()
    out_var_op = out_var_op.stack()
    out_dist_op = out_dist_op.stack()
    out_reg_op = out_reg_op.stack()

    disc_loss = tf.reduce_mean(out_loss_op)
    l_var = tf.reduce_mean(out_var_op)
    l_dist = tf.reduce_mean(out_dist_op)
    l_reg = tf.reduce_mean(out_reg_op)


    return disc_loss, l_var, l_dist, l_reg



def new_loss(prediction, correct_label, feature_dim,point_xyz,
                               alpha, beta):
    xyz = point_xyz[:, :3]
    xyz = tf.reshape(xyz, [-1, 3])
    reshape_pred = tf.reshape(prediction, [-1, feature_dim])

    unique_lables, unique_ids, counts = tf.unique_with_counts(correct_label)
    counts = tf.cast(counts, dtype=tf.float32)
    num_instances = tf.size(unique_lables)

    # å°†å±äºåŒä¸€ä¸ªå®ä¾‹çš„ç‚¹æ‰€é¢„æµ‹å‡ºæ¥çš„ç‰¹å¾å‘é‡ç›¸åŠ ï¼Œç”Ÿæˆä¸€ä¸ªnum_instances*5çš„çŸ©é˜?
    segmented_sum = tf.unsorted_segment_sum(reshape_pred, unique_ids, num_instances)

    #æ¯ä¸ªå®ä¾‹çš„ç‰¹å¾å‘é‡çš„å¹³å‡å€?
    s = tf.div(segmented_sum, tf.reshape(counts, (-1, 1)))
    #å°†è¿™ä¸ªå¹³å‡å€¼èµ‹ç»™æ¯ä¸€ä¸ªå®ä¾‹ä¸­çš„ç‚¹
    s_expand = tf.gather(s, unique_ids)

    tmp_distance = reshape_pred - s_expand
    distance = tf.norm(tmp_distance, ord=1, axis=1)
    distance = tf.subtract(distance, alpha)
    distance = tf.clip_by_value(distance, 0., distance)
    distance = tf.square(distance)

    mu_sum = tf.unsorted_segment_sum(xyz, unique_ids, num_instances)
    mu = tf.div(mu_sum, tf.reshape(counts, (-1, 1)))
    mu_expand = tf.gather(mu, unique_ids)

    tmp_spatial_distance = xyz - mu_expand
    spatial_distance = tf.norm(tmp_spatial_distance, ord=1, axis=1)
    spatial_distance = tf.reciprocal(1 + tf.exp(-spatial_distance))

    loss_intra = tf.unsorted_segment_sum(tf.multiply(spatial_distance, distance), unique_ids, num_instances)
    loss_intra = tf.norm(loss_intra, ord=1, axis=0)
    #num_instances = tf.to_float(num_instances)
    loss_intra = tf.div(loss_intra, tf.cast(num_instances, tf.float32))

    #å°†åŸæ¥çš„num_instances*5çš„çŸ©é˜µï¼Œè¡Œæ‰©å±•num_instanceså€ï¼Œåˆ—ä¸å?
    s_d = tf.tile(s, [num_instances, 1])
    s_b = tf.tile(s, [1, num_instances])
    s_b = tf.reshape(s_b, (num_instances * num_instances, feature_dim))

    s_diff = tf.subtract(s_b, s_d)

    eye = tf.eye(num_instances)
    zero = tf.zeros(1, dtype=tf.float32)
    diff_cluster_mask = tf.equal(eye, zero)
    diff_cluster_mask = tf.reshape(diff_cluster_mask, [-1])

    s_diff_bool = tf.boolean_mask(s_diff, diff_cluster_mask)

    s_norm = tf.norm(s_diff_bool, ord=1, axis=1)
    loss_inter = tf.subtract(beta, s_norm)
    #loss_inter = tf.square(beta - s_norm)
    loss_inter = tf.clip_by_value(loss_inter, 0., loss_inter)
    loss_inter = tf.square(loss_inter)
    #loss_inter = tf.norm(loss_inter, ord=1, axis=0)
    loss_inter = tf.reduce_mean(loss_inter)
    #å®ä¾‹ä¸ªæ•°åœ¨ä¸€ä¸ªå—é‡Œå¯èƒ½åªæœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥ä¹‹å‰num_instances-1å¯èƒ½ä¸?
    #loss_inter = tf.div(loss_inter, tf.cast(num_instances, tf.float32) * (tf.cast(num_instances, tf.float32) - 0.99))
    def rt_0(): return 0.
    def rt_l(): return loss_inter
    loss_inter = tf.cond(tf.equal(1, num_instances), rt_0, rt_l)

    loss = loss_intra + loss_inter
    return loss, loss_intra, loss_inter


def new_batch_loss(prediction, correct_label, feature_dim, point_xyz,
                   alpha, beta):
    def cond(label, batch, out_loss, out_loss_intra, out_loss_inter, i):
        return tf.less(i, tf.shape(batch)[0])

    def body(label, batch, out_loss, out_loss_intra, out_loss_inter, i):
        loss, loss_intra, loss_inter= new_loss(prediction[i], correct_label[i], feature_dim,
                                                point_xyz[i], alpha, beta)

        out_loss = out_loss.write(i, loss)
        out_loss_intra = out_loss_intra.write(i, loss_intra)
        out_loss_inter = out_loss_inter.write(i, loss_inter)

        return label, batch, out_loss, out_loss_intra, out_loss_inter, i + 1

    out_ta_loss = tf.TensorArray(dtype=tf.float32,
                                 size=0,
                                 dynamic_size=True)
    out_ta_loss_intra = tf.TensorArray(dtype=tf.float32,
                                       size=0,
                                       dynamic_size=True)
    out_ta_loss_inter = tf.TensorArray(dtype=tf.float32,
                                       size=0,
                                       dynamic_size=True)


    _, _, out_loss_op, out_loss_intra_op, out_loss_inter_op, _ = tf.while_loop(cond, body, [correct_label,
                                                                                            prediction,
                                                                                            out_ta_loss,
                                                                                            out_ta_loss_intra,
                                                                                            out_ta_loss_inter,
                                                                                            0])
    sa_loss_op = out_loss_op.stack()
    sa_loss_intra_op = out_loss_intra_op.stack()
    sa_loss_inter_op = out_loss_inter_op.stack()


    sa_loss = tf.reduce_mean(sa_loss_op)
    sa_loss_intra = tf.reduce_mean(sa_loss_intra_op)
    sa_loss_inter = tf.reduce_mean(sa_loss_inter_op)

    return sa_loss, sa_loss_intra, sa_loss_inter

def new_loss_1(prediction, correct_label, feature_dim,point_xyz,
                               alpha, beta, gama):
    xyz = point_xyz[:, :3]
    xyz = tf.reshape(xyz, [-1, 3])
    reshape_pred = tf.reshape(prediction, [-1, feature_dim])

    unique_lables, unique_ids, counts = tf.unique_with_counts(correct_label)
    counts = tf.cast(counts, dtype=tf.float32)
    num_instances = tf.size(unique_lables)

    # å°†å±äºåŒä¸€ä¸ªå®ä¾‹çš„ç‚¹æ‰€é¢„æµ‹å‡ºæ¥çš„ç‰¹å¾å‘é‡ç›¸åŠ ï¼Œç”Ÿæˆä¸€ä¸ªnum_instances*5çš„çŸ©é˜?
    segmented_sum = tf.unsorted_segment_sum(reshape_pred, unique_ids, num_instances)

    #æ¯ä¸ªå®ä¾‹çš„ç‰¹å¾å‘é‡çš„å¹³å‡å€?
    s = tf.div(segmented_sum, tf.reshape(counts, (-1, 1)))
    #å°†è¿™ä¸ªå¹³å‡å€¼èµ‹ç»™æ¯ä¸€ä¸ªå®ä¾‹ä¸­çš„ç‚¹
    s_expand = tf.gather(s, unique_ids)

    tmp_distance = reshape_pred - s_expand
    distance = tf.norm(tmp_distance, ord=1, axis=1)
    distance = tf.subtract(distance, alpha)
    distance = tf.clip_by_value(distance, 0., distance)
    distance = tf.square(distance)

    mu_sum = tf.unsorted_segment_sum(xyz, unique_ids, num_instances)
    mu = tf.div(mu_sum, tf.reshape(counts, (-1, 1)))
    mu_expand = tf.gather(mu, unique_ids)

    tmp_spatial_distance = xyz - mu_expand
    spatial_distance = tf.norm(tmp_spatial_distance, ord=1, axis=1)
    spatial_distance = tf.reciprocal(1 + tf.exp(-spatial_distance))

    loss_intra = tf.unsorted_segment_sum(tf.multiply(spatial_distance, distance), unique_ids, num_instances)
    loss_intra = tf.norm(loss_intra, ord=1, axis=0)
    #num_instances = tf.to_float(num_instances)
    loss_intra = tf.div(loss_intra, tf.cast(num_instances, tf.float32))

    #å°†åŸæ¥çš„num_instances*5çš„çŸ©é˜µï¼Œè¡Œæ‰©å±•num_instanceså€ï¼Œåˆ—ä¸å?
    s_d = tf.tile(s, [num_instances, 1])
    s_b = tf.tile(s, [1, num_instances])
    s_b = tf.reshape(s_b, (num_instances * num_instances, feature_dim))

    s_diff = tf.subtract(s_b, s_d)

    eye = tf.eye(num_instances)
    zero = tf.zeros(1, dtype=tf.float32)
    diff_cluster_mask = tf.equal(eye, zero)
    diff_cluster_mask = tf.reshape(diff_cluster_mask, [-1])

    s_diff_bool = tf.boolean_mask(s_diff, diff_cluster_mask)

    s_norm = tf.norm(s_diff_bool, ord=1, axis=1)
    loss_inter = tf.subtract(beta, s_norm)
    #loss_inter = tf.square(beta - s_norm)
    loss_inter = tf.clip_by_value(loss_inter, 0., loss_inter)
    loss_inter = tf.square(loss_inter)
    #loss_inter = tf.norm(loss_inter, ord=1, axis=0)
    loss_inter = tf.reduce_mean(loss_inter)
    #å®ä¾‹ä¸ªæ•°åœ¨ä¸€ä¸ªå—é‡Œå¯èƒ½åªæœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥ä¹‹å‰num_instances-1å¯èƒ½ä¸?
    #loss_inter = tf.div(loss_inter, tf.cast(num_instances, tf.float32) * (tf.cast(num_instances, tf.float32) - 0.99))
    def rt_0(): return 0.
    def rt_l(): return loss_inter
    loss_inter = tf.cond(tf.equal(1, num_instances), rt_0, rt_l)

    l_reg = tf.reduce_mean(tf.norm(s, ord=1, axis=1))

    loss = loss_intra + loss_inter + (gama * l_reg)
    return loss, loss_intra, loss_inter, l_reg


def new_batch_loss_1(prediction, correct_label, feature_dim, point_xyz,
                     alpha, beta, gama):
    def cond(label, batch, out_loss, out_loss_intra, out_loss_inter, out_l_reg, i):
        return tf.less(i, tf.shape(batch)[0])

    def body(label, batch, out_loss, out_loss_intra, out_loss_inter, out_l_reg, i):
        loss, loss_intra, loss_inter, l_reg = new_loss_1(prediction[i], correct_label[i], feature_dim,
                                                         point_xyz[i], alpha, beta, gama)

        out_loss = out_loss.write(i, loss)
        out_loss_intra = out_loss_intra.write(i, loss_intra)
        out_loss_inter = out_loss_inter.write(i, loss_inter)
        out_l_reg = out_l_reg.write(i, l_reg)

        return label, batch, out_loss, out_loss_intra, out_loss_inter, out_l_reg, i + 1

    out_ta_loss = tf.TensorArray(dtype=tf.float32,
                                 size=0,
                                 dynamic_size=True)
    out_ta_loss_intra = tf.TensorArray(dtype=tf.float32,
                                       size=0,
                                       dynamic_size=True)
    out_ta_loss_inter = tf.TensorArray(dtype=tf.float32,
                                       size=0,
                                       dynamic_size=True)
    out_ta_l_reg = tf.TensorArray(dtype=tf.float32,
                                  size=0,
                                  dynamic_size=True)

    _, _, out_loss_op, out_loss_intra_op, out_loss_inter_op, out_l_reg_op, _ = tf.while_loop(cond, body, [correct_label,
                                                                                                          prediction,
                                                                                                          out_ta_loss,
                                                                                                          out_ta_loss_intra,
                                                                                                          out_ta_loss_inter,
                                                                                                          out_ta_l_reg,
                                                                                                          0])
    sa_loss_op = out_loss_op.stack()
    sa_loss_intra_op = out_loss_intra_op.stack()
    sa_loss_inter_op = out_loss_inter_op.stack()
    sa_l_reg_op = out_l_reg_op.stack()

    sa_loss = tf.reduce_mean(sa_loss_op)
    sa_loss_intra = tf.reduce_mean(sa_loss_intra_op)
    sa_loss_inter = tf.reduce_mean(sa_loss_inter_op)
    sa_l_reg = tf.reduce_mean(sa_l_reg_op)

    return sa_loss, sa_loss_intra, sa_loss_inter, sa_l_reg

def new_loss_2(prediction, correct_label, feature_dim, num_ins,
                               delta_v, delta_d, param_var, param_dist, param_reg):
    reshaped_pred = tf.reshape(prediction, [-1, feature_dim])

    ### Count instances
    unique_labels, unique_id, counts = tf.unique_with_counts(correct_label)
    counts = tf.cast(counts, tf.float32)
    num_instances = tf.size(unique_labels)
    segmented_sum = tf.unsorted_segment_sum(reshaped_pred, unique_id, num_instances)
    mu = tf.div(segmented_sum, tf.reshape(counts, (-1, 1)))
    mu_expand = tf.gather(mu, unique_id)

    ### Calculate l_var
    tmp_distance = reshaped_pred - mu_expand
    distance = tf.norm(tmp_distance, ord=1, axis=1)
    distance = tf.subtract(distance, delta_v)
    distance = tf.clip_by_value(distance, 0., distance)
    distance = tf.square(distance)

    l_var = tf.unsorted_segment_sum(distance, unique_id, num_instances)
    l_var = tf.div(l_var, counts)
    l_var = tf.reduce_sum(l_var)
    l_var = tf.divide(l_var, tf.cast(num_instances, tf.float32))

    ### Calculate l_dist

    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])
    mu_band_rep = tf.tile(mu, [1, num_instances])
    mu_band_rep = tf.reshape(mu_band_rep, (num_instances * num_instances, feature_dim))

    mu_diff = tf.subtract(mu_band_rep, mu_interleaved_rep)

    eye = tf.eye(num_instances)
    zero = tf.zeros(1, dtype=tf.float32)
    diff_cluster_mask = tf.equal(eye, zero)
    diff_cluster_mask = tf.reshape(diff_cluster_mask, [-1])
    mu_diff_bool = tf.boolean_mask(mu_diff, diff_cluster_mask)

    mu_norm = tf.norm(mu_diff_bool, ord=1, axis=1)
    mu_norm = tf.subtract(2. * delta_d, mu_norm)
    mu_norm = tf.clip_by_value(mu_norm, 0., mu_norm)
    mu_norm = tf.square(mu_norm)

    l_dist = tf.reduce_mean(mu_norm)

    def rt_0(): return 0.

    def rt_l_dist(): return l_dist

    l_dist = tf.cond(tf.equal(1, num_instances), rt_0, rt_l_dist)

    ### Calculate l_reg
    l_reg = tf.reduce_mean(tf.norm(mu, ord=1, axis=1))
    num_ins_gt = tf.cast(num_instances, tf.float32)
    l_num = tf.abs(tf.subtract(num_ins, num_ins_gt))

    param_scale = 1.
    l_var = param_var * l_var
    l_dist = param_dist * l_dist
    l_reg = param_reg * l_reg
    l_num = 1.5 * l_num

    loss = param_scale * (l_var + l_dist + l_reg + l_num)

    return loss, l_var, l_dist, l_reg, l_num

def discriminative_loss_1(prediction, correct_label, feature_dim, num_ins,
                        delta_v, delta_d, param_var, param_dist, param_reg):
    ''' Iterate over a batch of prediction/label and cumulate loss
    :return: discriminative loss and its three components
    '''

    def cond(label, batch, out_loss, out_var, out_dist, out_reg, out_num, i):
        return tf.less(i, tf.shape(batch)[0])

    def body(label, batch, out_loss, out_var, out_dist, out_reg, out_num, i):
        disc_loss, l_var, l_dist, l_reg, l_num = new_loss_2(prediction[i], correct_label[i], feature_dim, num_ins,
                                                                     delta_v, delta_d, param_var, param_dist, param_reg)

        out_loss = out_loss.write(i, disc_loss)
        out_var = out_var.write(i, l_var)
        out_dist = out_dist.write(i, l_dist)
        out_reg = out_reg.write(i, l_reg)
        out_num = out_num.write(i, l_num)

        return label, batch, out_loss, out_var, out_dist, out_reg, out_num, i + 1

    # TensorArray is a data structure that support dynamic writing
    output_ta_loss = tf.TensorArray(dtype=tf.float32,
                                    size=0,
                                    dynamic_size=True)
    output_ta_var = tf.TensorArray(dtype=tf.float32,
                                   size=0,
                                   dynamic_size=True)
    output_ta_dist = tf.TensorArray(dtype=tf.float32,
                                    size=0,
                                    dynamic_size=True)
    output_ta_reg = tf.TensorArray(dtype=tf.float32,
                                   size=0,
                                   dynamic_size=True)
    output_ta_num = tf.TensorArray(dtype=tf.float32,
                                   size=0,
                                   dynamic_size=True)

    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, out_num_op, _ = tf.while_loop(cond, body, [correct_label,
                                                                                           prediction,
                                                                                           output_ta_loss,
                                                                                           output_ta_var,
                                                                                           output_ta_dist,
                                                                                           output_ta_reg,
                                                                                           output_ta_num,
                                                                                           0])
    out_loss_op = out_loss_op.stack()
    out_var_op = out_var_op.stack()
    out_dist_op = out_dist_op.stack()
    out_reg_op = out_reg_op.stack()
    out_num_op = out_num_op.stack()

    disc_loss = tf.reduce_mean(out_loss_op)
    l_var = tf.reduce_mean(out_var_op)
    l_dist = tf.reduce_mean(out_dist_op)
    l_reg = tf.reduce_mean(out_reg_op)
    l_num = tf.reduce_mean(out_num_op)

    return disc_loss, l_var, l_dist, l_reg, l_num


